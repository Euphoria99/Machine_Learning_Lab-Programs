# -*- coding: utf-8 -*-
"""4th-ANN-backpropagation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18VtXYPGw2fvj8JrCYL7tYhBDD5HYZD5v

4.**Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same using appropriate data sets.**
"""

import numpy as np

x = np.array(([2,9], [1,5], [3,6]), dtype = float )

y = np.array(([92], [86], [89]), dtype = float )

x = x/np.amax(x, axis=0)

y = y/ 100

def sigmoid(x):
    return 1/(1+ np.exp(-x))

def derivatives_sigmoid(x):
    return 1/(1+ np.exp(1 -x))

epoch = 7000

learning_rate =0.1

inputlayer_neurons =2

hiddenlayer_neurons =3

outputlayer_neurons =1

#weight of hidden layer
wh = np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons) )

#Bias of hidden layer
bh=np.random.uniform(size =(1,hiddenlayer_neurons))

#weight of output layer
wo=np.random.uniform(size =(hiddenlayer_neurons, outputlayer_neurons))

#Bias of output layer
bo = np.random.uniform(size =(1,outputlayer_neurons))

# training
for i in range(epoch):
    #sum of (input + weights in hidden layer) +bias of hidden
    net_h = np.dot(x,wh)+bh
    #apply activation function
    sigma_h = sigmoid(net_h)
    #input to O/P layer = (O/P of hidden layer * weight of O/P layer) + bias of O/P layer
    net_o = np.dot(sigma_h,wo) + bo
    #apply activation function
    output = sigmoid(net_o)
    
    
    #finding deltas (cost function implemtation)
    #delta of o/p layer
    deltaK=(y-output)*derivatives_sigmoid(output)
    #delta of hidden layer
    deltaH = deltaK.dot(wo.T)*derivatives_sigmoid(sigma_h)
    #update the weights
    wo = wo +sigma_h.T.dot(deltaK)* learning_rate
    wh = wh + x.T.dot(deltaH) * learning_rate
    
    error =sum(deltaK)**2 / len(deltaK)
    
    print('Epoach -> {0}, lrate -> {1}, error -> {2}' . format(i, learning_rate, error))

print("Input: \n" + str(x))

print("Actual Output: \n" + str(y))

print("Predicted Output: \n" , output)